{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DAT264x: Microsoft Professional Capstone : Artificial Intelligence - Dec 2019 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best I could restore for the archived website..... :-(\n",
    "\n",
    "\n",
    "**DAT264x: Identifying Appliances from Energy Use Spectrograms.\n",
    "Hosted By Microsoft **\n",
    "\n",
    "* According to a 2017 report, the U.S. Energy Information Administration projects a 28% increase in world energy consumption by 2040. And the energy sector is a major contributor to climate change. For example, energy production and use accounts for more than 84% of U.S. greenhouse gas emissions.\n",
    "* Increasing the efficiency of energy consumption has benefits for consumers, providers, and the environment. With an increasing number of IoT devices coming online in the energy sector, there is more and more data that can be used to monitor and track energy consumption. Ultimately, this type of data can be used to provide consumers and businesses with recommendations on ways to save energy, lower costs, and help the planet.\n",
    "* In this challenge, you will use standard AI tools to identify 11 different types of appliances from their electric signatures, quantified by current and voltage measurements.\n",
    "* This plug load dataset contains current and voltage measurements sampled at 30 kHz from 11 different appliance types present in more than 60 households in Pittsburgh, Pennsylvania.  Plus load refers to the energy used by products that are powered by means of an ordinary AC plug (i.e., plugged into an outlet). \n",
    "* For each appliance, plug load measurements were post-processed to extract a three-second-long window of measurements of current and voltage. For some observations, the window contains both the startup transient state (turning the appliance on) as well as the steady-state operation (once the appliance is running). \n",
    "* For others, the window only contains the steady-state operation. The observations were then transformed into two spectrograms, one for current, and one for voltage.\n",
    "*  A spectrogram is a visual representation of the various frequencies of sound as they vary with time. The x-axis represents time (3 seconds in our case), and the y-axis represents frequency (measured in Hz). The colors indicate the amplitude of a particular frequency at a particular time (i.e., how loud it is). \n",
    "* We're measuring amplitude in decibels,with 0 being the loudest, and -80 being the softest. So in the example spectrogram below, lower frequencies are louder than higher frequencies. \n",
    "\n",
    "*Our spectrograms tend to have horizontal lines given that we are capturing appliances in their steady-state. In other words, the amplitudes of various frequencies are fairly constant over time.*\n",
    "\n",
    "Report: https://www.eia.gov/todayinenergy/detail.php?id=32912\n",
    "\n",
    "Under the hood, this process:\n",
    "* Takes the [Fourier transform](http://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/) of a windowed excerpt of the raw signal, in order to decompose the signal into its consistuent frequencies.\n",
    "* To learn more about Fourier transforms, check out this awesome tutorial by 3Blue1Brown: [But what is the Fourier Transform](https://www.youtube.com/watch?v=spUNpyF58BY). (PS:- Really reallt helpful)\n",
    "* Maps the powers of the spectrum onto the [mel scale](https://en.wikipedia.org/wiki/Mel_scale). The mel scale is a perceptual scale where pitches are judged to be equal in distance from one another based on the human ear. \n",
    "* Takes the logs of the power (amplitude squared) at each of the mel frequencies to [convert to decibel units](https://librosa.github.io/librosa/generated/librosa.core.power_to_db.html). \n",
    "* Plots and saves the resulting image. \n",
    "* There is a lot of useful information encoded in these spectrograms. Now it's time to use your deep learning skills to parse out which patterns correspond to which types of appliances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bijli-wala-project/submission_format.csv\n",
      "/kaggle/input/bijli-wala-project/image_test.data\n",
      "/kaggle/input/bijli-wala-project/image_train.data\n",
      "/kaggle/input/bijli-wala-project/image_train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from PIL import Image, ImageFilter\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration data \n",
    "image_height = 128//1\n",
    "image_width = 2*176//1\n",
    "training_image_count = 576\n",
    "testing_image_count = 384\n",
    "classes_count = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>appliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1577</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  appliance\n",
       "0  1576          0\n",
       "1  1577          0\n",
       "2  1578          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root_path = '../input/bijli-wala-project/'\n",
    "sub = pd.read_csv(\"../input/bijli-wala-project/submission_format.csv\")\n",
    "sub.to_csv('submission_results.csv', index=False)\n",
    "sub.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_data_file_path = data_root_path + 'image_train.data'\n",
    "training_labels_data_file_path = data_root_path + 'image_train_labels.csv'\n",
    "testing_data_file_path = data_root_path + 'image_test.data'\n",
    "\n",
    "testing_submission_file_path = data_root_path + 'submission_format.csv'\n",
    "submission_results_file_path =  '/kaggle/working/' + 'submission_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_train_image(p_img1, p_img2) :\n",
    "    #Stacks images horizontally (i.e. one afer another on width axis)\n",
    "    img_merge_data = np.hstack([np.asarray(p_img1), np.asarray(p_img2)])\n",
    "    img_merge = Image.fromarray( img_merge_data )\n",
    "        \n",
    "    return img_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data(p_image) :\n",
    "    #Generates image data from the received image object\n",
    "    width, height = p_image.size\n",
    "    data = np.asarray(p_image).reshape(height*width)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainining_images_data_file(p_input_data_file_path, p_training_data_file_path):\n",
    "    training_labels_file_path = 'train_labels.csv'\n",
    "    labels = None\n",
    "\n",
    "    with open(p_training_data_file_path, 'w+b') as data_file :\n",
    "        with ZipFile(p_input_data_file_path) as data_zip:\n",
    "            with data_zip.open(training_labels_file_path) as train_labels_file:\n",
    "                content = train_labels_file.read()\n",
    "                with BytesIO(content) as io_content:\n",
    "                    train_labels = pd.read_csv(io_content)\n",
    "                    max_count = train_labels.shape[0]    \n",
    "                    labels = np.zeros(max_count)\n",
    "                    count = 0\n",
    "\n",
    "                    for _, row in train_labels.iterrows() :\n",
    "                        with data_zip.open('train/' + str(row[\"id\"]) + \"_c.png\") as c_file :\n",
    "                            with BytesIO(c_file.read()) as input_buffer:\n",
    "                                c_image = Image.open(input_buffer).convert(\"L\")\n",
    "\n",
    "                        with data_zip.open('train/' + str(row[\"id\"]) + \"_v.png\") as v_file :\n",
    "                            with BytesIO(v_file.read()) as input_buffer:\n",
    "                                v_image = Image.open(input_buffer).convert(\"L\")\n",
    "\n",
    "                        image_data = get_image_data(compose_train_image(c_image, v_image))\n",
    "                        labels[count] = row[\"appliance\"]\n",
    "                        data_file.write(image_data)\n",
    "\n",
    "                        count = count + 1       \n",
    "\n",
    "    return labels[:count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_labels(p_labels, p_labels_data_file_path) :\n",
    "    classes = pd.DataFrame(p_labels.astype(int))\n",
    "    classes.to_csv(p_labels_data_file_path, header=None)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_images_data_file(p_input_data_file_path, p_testing_data_file_path):\n",
    "    submission_format_file_path = 'submission_format.csv'\n",
    "\n",
    "    with open(p_testing_data_file_path, 'w+b') as data_file :\n",
    "        with ZipFile(p_input_data_file_path) as data_zip:\n",
    "            with data_zip.open(submission_format_file_path) as submission_format_file:\n",
    "                content = submission_format_file.read()\n",
    "                with BytesIO(content) as io_content:\n",
    "                    submission_indexes = pd.read_csv(io_content)\n",
    "                    count = 0\n",
    "\n",
    "                    for _, row in submission_indexes.iterrows() :\n",
    "                        with data_zip.open('test/' + str(row[\"id\"]) + \"_c.png\") as c_file :\n",
    "                            with BytesIO(c_file.read()) as input_buffer:\n",
    "                                c_image = Image.open(input_buffer).convert(\"L\")\n",
    "\n",
    "                        with data_zip.open('test/' + str(row[\"id\"]) + \"_v.png\") as v_file :\n",
    "                            with BytesIO(v_file.read()) as input_buffer:\n",
    "                                v_image = Image.open(input_buffer).convert(\"L\")\n",
    "\n",
    "                        image_data = get_image_data(compose_train_image(c_image, v_image))\n",
    "                        data_file.write(image_data)\n",
    "\n",
    "                        count = count + 1       \n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_submission(p_input_data_file_path, p_testing_submission_file_path) :\n",
    "    submission_format_file_path = 'submission_format.csv'\n",
    "\n",
    "    with ZipFile(p_input_data_file_path) as data_zip:\n",
    "        with data_zip.open(submission_format_file_path) as submission_format_file:\n",
    "            content = submission_format_file.read()\n",
    "            with BytesIO(content) as io_content:\n",
    "                submission_indexes = pd.read_csv(io_content)\n",
    "                submission_indexes.to_csv(p_testing_submission_file_path, index=False)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() :\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    #create training data\n",
    "    logging.info('Creating training data ...')\n",
    "    training_labels  = create_trainining_images_data_file(input_data_file_path, training_image_data_file_path)\n",
    "    create_training_labels(training_labels, training_labels_data_file_path)\n",
    "    logging.info(\"Processed training images count: %d\" % training_labels.shape[0])\n",
    "    logging.info('Creating training data DONE')\n",
    "\n",
    "    logging.info('Creating testing data ...')\n",
    "    testing_count = create_testing_images_data_file(input_data_file_path, testing_data_file_path)\n",
    "    create_testing_submission(input_data_file_path, testing_submission_file_path)\n",
    "    logging.info(\"Processed testing images count: %d\" % testing_count)\n",
    "    logging.info('Creating testing data DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(p_image_data_file_path, p_position, p_image_width, p_image_height) :\n",
    "    with open(p_image_data_file_path, \"rb\") as image_file :\n",
    "        image_file.seek(p_position * p_image_height* p_image_width)\n",
    "        data = image_file.read(p_image_height * p_image_width)\n",
    "        data_b = np.frombuffer(data, dtype=np.uint8)\n",
    "\n",
    "    return np.asarray(data_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(p_images, p_image_width, p_image_height) :\n",
    "\n",
    "    #reshape according to inputs accepted by a Conv2d layer\n",
    "    processed_images = p_images.reshape(p_images.shape[0], p_image_height, p_image_width, 1)\n",
    "\n",
    "    #data normalization to max value (0-255 grayscale values)\n",
    "    processed_images = (processed_images * 1.0) /255\n",
    " \n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(p_labels_file_path) :\n",
    "  \n",
    "    labels = pd.read_csv(p_labels_file_path, header= None)\n",
    "    labels.columns = [\"id\", \"label\"]\n",
    "  \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labels(p_labels) :\n",
    "    processed_labels = LabelBinarizer().fit_transform(p_labels)\n",
    "    \n",
    "    return processed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_set(\n",
    "    p_image_training_data_file_path, \n",
    "    p_labels_file_path, \n",
    "    p_train_set_size, \n",
    "    p_image_width, \n",
    "    p_image_height\n",
    ") :\n",
    "    labels = read_labels(p_labels_file_path)\n",
    "    \n",
    "    labels_batch = np.zeros(p_train_set_size)\n",
    "    labels_batch = labels[\"label\"][0:p_train_set_size].values\n",
    "\n",
    "    images_batch = []\n",
    "  \n",
    "    for i in range(0, p_train_set_size) :\n",
    "        image_data = read_image(p_image_training_data_file_path, i, p_image_width, p_image_height)\n",
    "        images_batch.append(image_data.reshape(p_image_height, p_image_width))\n",
    "  \n",
    "    train_labels_processed = process_labels(labels_batch)\n",
    "  \n",
    "    train_images_processed = process_images(np.array(images_batch), p_image_width, p_image_height)\n",
    "  \n",
    "    return train_labels_processed, train_images_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_set(\n",
    "    p_test_image_data_file_path, \n",
    "    p_test_set_size, \n",
    "    p_image_width, \n",
    "    p_image_height\n",
    ") :\n",
    "    images_batch = []\n",
    "\n",
    "    for i in range(0, p_test_set_size) :\n",
    "        image_data = read_image(p_test_image_data_file_path, i, p_image_width, p_image_height)\n",
    "        images_batch.append(image_data.reshape(p_image_height, p_image_width))\n",
    "\n",
    "    test_images_processed = process_images(np.array(images_batch), p_image_width, p_image_height)\n",
    "\n",
    "    return test_images_processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(p_image_width, p_image_height, p_num_classes) :\n",
    "    input_shape = (p_image_height, p_image_width, 1)\n",
    "\n",
    "    #we will use a sequential model for training \n",
    "    model = Sequential()\n",
    "\t\n",
    "    #CONV 3x3x32 => RELU => NORMALIZATION => MAX POOL 3x3 block\n",
    "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=input_shape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "    #CONV 3x3x64 => RELU => NORMALIZATION => MAX POOL 2x2 block\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #CONV 3x3x128 => RELU => NORMALIZATION => MAX POOL 2x2 block\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #FLATTEN => DENSE 1024 => RELU => NORMALIZATION block\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #final DENSE => SOFTMAX block for multi-label classification\n",
    "    model.add(Dense(p_num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    #using categorical_crossentropy loss function with adam optimizer\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    p_model, \n",
    "    p_training_image_data, \n",
    "    p_trainging_labels, \n",
    "    p_batch_size = 64, \n",
    "    p_epochs_to_train = 50, \n",
    "    p_verbose_level = 2\n",
    ") : \n",
    "    p_model.fit(\n",
    "        x = p_training_image_data, \n",
    "        y = p_trainging_labels, \n",
    "        batch_size = p_batch_size, \n",
    "        epochs = p_epochs_to_train,\n",
    "        shuffle = True,\n",
    "        verbose = p_verbose_level    \n",
    "    )\n",
    "    \n",
    "    return p_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(p_model, p_test_image_data, p_batch_size = 32) :\n",
    "    labels = p_model.predict_classes(p_test_image_data, p_batch_size)\n",
    "  \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(\n",
    "    p_testing_submission_file_path, \n",
    "    p_submission_results_file_path, \n",
    "    p_results\n",
    ") :\n",
    "    submission_structure = pd.read_csv(p_testing_submission_file_path)\n",
    "    submission_structure['appliance'] = p_results\n",
    "    submission_structure.to_csv(p_submission_results_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 36s - loss: 2.1606 - accuracy: 0.5938\n",
      "Epoch 2/50\n",
      " - 34s - loss: 0.3981 - accuracy: 0.8594\n",
      "Epoch 3/50\n",
      " - 34s - loss: 0.2345 - accuracy: 0.9392\n",
      "Epoch 4/50\n",
      " - 34s - loss: 0.1506 - accuracy: 0.9618\n",
      "Epoch 5/50\n",
      " - 35s - loss: 0.0864 - accuracy: 0.9722\n",
      "Epoch 6/50\n",
      " - 34s - loss: 0.0620 - accuracy: 0.9826\n",
      "Epoch 7/50\n",
      " - 35s - loss: 0.0346 - accuracy: 0.9931\n",
      "Epoch 8/50\n",
      " - 34s - loss: 0.0166 - accuracy: 0.9983\n",
      "Epoch 9/50\n",
      " - 34s - loss: 0.0096 - accuracy: 0.9983\n",
      "Epoch 10/50\n",
      " - 35s - loss: 0.0068 - accuracy: 0.9983\n",
      "Epoch 11/50\n",
      " - 34s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      " - 35s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      " - 34s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      " - 35s - loss: 9.7747e-04 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      " - 34s - loss: 8.8765e-04 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      " - 37s - loss: 7.8128e-04 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      " - 34s - loss: 8.0407e-04 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      " - 34s - loss: 5.4119e-04 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      " - 34s - loss: 4.4434e-04 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      " - 34s - loss: 3.2875e-04 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      " - 34s - loss: 3.8690e-04 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      " - 34s - loss: 3.1925e-04 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      " - 34s - loss: 2.7379e-04 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      " - 35s - loss: 3.1880e-04 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      " - 34s - loss: 2.8703e-04 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      " - 35s - loss: 1.9317e-04 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      " - 34s - loss: 3.7459e-04 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      " - 35s - loss: 2.8895e-04 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      " - 35s - loss: 2.0113e-04 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      " - 34s - loss: 2.0109e-04 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      " - 34s - loss: 1.6184e-04 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      " - 34s - loss: 1.6555e-04 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      " - 35s - loss: 1.5663e-04 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      " - 34s - loss: 2.1516e-04 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      " - 34s - loss: 1.3221e-04 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      " - 34s - loss: 1.4661e-04 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      " - 34s - loss: 1.0881e-04 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      " - 35s - loss: 1.2843e-04 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      " - 34s - loss: 1.7511e-04 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      " - 35s - loss: 1.3743e-04 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      " - 34s - loss: 9.9388e-05 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      " - 36s - loss: 1.2887e-04 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      " - 35s - loss: 1.3999e-04 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      " - 34s - loss: 1.1077e-04 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      " - 35s - loss: 9.3249e-05 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      " - 34s - loss: 1.6432e-04 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      " - 35s - loss: 1.2229e-04 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      " - 35s - loss: 8.3919e-05 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      " - 34s - loss: 9.2047e-05 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      " - 35s - loss: 2.2449e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    #prepare training data\n",
    "    logging.info('Reading training data ...')\n",
    "    train_labels, train_images = generate_train_set(\n",
    "        training_image_data_file_path, \n",
    "        training_labels_data_file_path, \n",
    "        training_image_count, \n",
    "        image_width, \n",
    "        image_height\n",
    "    )\n",
    "    logging.info('Reading training data DONE')\n",
    "    \n",
    "    #create and train model\n",
    "    logging.info('Creating model ...')\n",
    "    model = create_model (image_width, image_height, classes_count)\n",
    "    logging.info('Creating model DONE')\n",
    "\n",
    "    logging.info('Training model ... ')\n",
    "    model = train_model(model, train_images, train_labels, p_epochs_to_train = 50)\n",
    "    logging.info('Training model DONE')\n",
    "    \n",
    "    #create test data\n",
    "    logging.info('Reading testing data ...')\n",
    "    test_images = generate_test_set(\n",
    "      testing_data_file_path, \n",
    "      testing_image_count, \n",
    "      image_width, \n",
    "      image_height\n",
    "    )\n",
    "    logging.info('Reading testing data DONE')\n",
    "    \n",
    "    #predict labels for test data\n",
    "    logging.info('Predicting test data classes ...')\n",
    "    result = predict_labels(model, test_images)\n",
    "    logging.info('Predicting test data classes DONE')\n",
    "    \n",
    "    #write results\n",
    "    logging.info('Writing results ...')\n",
    "    write_results(\n",
    "        testing_submission_file_path, \n",
    "        submission_results_file_path, \n",
    "        result\n",
    "    )\n",
    "    logging.info('Writing results DONE')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ibb.co/3fWvrXQ/Certificate.jpg\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I used 'bijli-wala-project' which in Hindi means 'That project with electricity thingy' which was the only project at that time........ But wanted to make it fun and happy ;-)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
